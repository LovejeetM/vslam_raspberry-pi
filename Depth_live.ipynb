{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca49b26a",
   "metadata": {},
   "source": [
    "Trying out different models and their performance for the live depth mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b816a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch torchvision accelerate aiortc aiohttp Pillow --quiet\n",
    "\n",
    "\n",
    "import asyncio, cv2, numpy as np, aiohttp, logging, base64, torch\n",
    "from aiortc import RTCPeerConnection, RTCSessionDescription\n",
    "from aiortc.mediastreams import MediaStreamError\n",
    "from IPython.display import display, HTML\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n",
    "# from transformers import GLPNImageProcessor, GLPNForDepthEstimation      # For glpn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89194b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "WHEP_URL = \"https://(          Your Url          )/whep\"\n",
    "DISPLAY_FPS =  30   #  15\n",
    "\n",
    "logging.getLogger(\"libav\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"aiortc.codecs.h264\").setLevel(logging.ERROR)\n",
    "\n",
    "try:\n",
    "    checkpoint = \"LiheYoung/depth-anything-small-hf\"       \n",
    "    # \"vinvino02/glpn-kitti\"   \"vinvino02/glpn-nyu\"    \"apple/DepthPro-hf\"   \"LiheYoung/depth-anything-large-hf\"\n",
    "    # \"LiheYoung/depth-anything-small-hf\"    \"Intel/dpt-large\"\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
    "    print(f\"Loading model on device: '{device}' with dtype: {dtype}\")\n",
    "\n",
    "    image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "    depth_model = AutoModelForDepthEstimation.from_pretrained(\n",
    "        checkpoint,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    print(\"Model and processor loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63839581",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebRTCStreamViewer:\n",
    "    def __init__(self, whep_url, processor, model, device, dtype):\n",
    "        self.whep_url = whep_url\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.pc = RTCPeerConnection()\n",
    "        self.done = asyncio.Event()\n",
    "        self.lock = asyncio.Lock()\n",
    "        self.latest_frame = None\n",
    "\n",
    "        @self.pc.on(\"track\")\n",
    "        async def on_track(track):\n",
    "            if track.kind == \"video\":\n",
    "                asyncio.create_task(self._frame_receiver_task(track))\n",
    "\n",
    "        @self.pc.on(\"connectionstatechange\")\n",
    "        async def on_connectionstatechange():\n",
    "            if self.pc.connectionState in [\"failed\", \"closed\", \"disconnected\"]:\n",
    "                self.done.set()\n",
    "\n",
    "    async def _frame_receiver_task(self, track):\n",
    "        while not self.done.is_set():\n",
    "            try:\n",
    "                frame = await track.recv()\n",
    "                img = frame.to_ndarray(format=\"bgr24\")\n",
    "                async with self.lock:\n",
    "                    self.latest_frame = img\n",
    "            except MediaStreamError:\n",
    "                return\n",
    "\n",
    "    async def _display_loop_task(self):\n",
    "        display_handle = display(HTML('<img>'), display_id=True)\n",
    "\n",
    "        while not self.done.is_set():\n",
    "            frame_to_display = None\n",
    "            async with self.lock:\n",
    "                if self.latest_frame is not None:\n",
    "                    frame_to_display = self.latest_frame.copy()\n",
    "\n",
    "            if frame_to_display is not None:\n",
    "                original_h, original_w, _ = frame_to_display.shape\n",
    "                \n",
    "                rgb_image = Image.fromarray(cv2.cvtColor(frame_to_display, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "                inputs = self.processor(images=rgb_image, return_tensors=\"pt\")\n",
    "                pixel_values = inputs.pixel_values.to(self.device, dtype=self.dtype)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(pixel_values)\n",
    "                    predicted_depth = outputs.predicted_depth\n",
    "\n",
    "                prediction = torch.nn.functional.interpolate(\n",
    "                    predicted_depth.unsqueeze(1),\n",
    "                    size=(original_h, original_w),\n",
    "                    mode=\"bicubic\",\n",
    "                    align_corners=False,\n",
    "                ).squeeze()\n",
    "\n",
    "                output = prediction.cpu().numpy()\n",
    "                formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n",
    "                depth_colormap = cv2.applyColorMap(formatted, cv2.COLORMAP_J)\n",
    "\n",
    "                combined_frame = np.hstack((frame_to_display, depth_colormap))\n",
    "\n",
    "                _, buffer = cv2.imencode('.jpg', combined_frame)\n",
    "                b64_str = base64.b64encode(buffer).decode('utf-8')\n",
    "                data_url = f\"data:image/jpeg;base64,{b64_str}\"\n",
    "                display_handle.update(HTML(f'<img src=\"{data_url}\" style=\"width: 80%;\" />'))\n",
    "\n",
    "            await asyncio.sleep(1 / DISPLAY_FPS)\n",
    "\n",
    "    async def run(self):\n",
    "        self.pc.addTransceiver(\"video\", direction=\"recvonly\")\n",
    "        offer = await self.pc.createOffer()\n",
    "        await self.pc.setLocalDescription(offer)\n",
    "\n",
    "        try:\n",
    "            display_task = asyncio.create_task(self._display_loop_task())\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                async with session.post(self.whep_url, data=self.pc.localDescription.sdp,\n",
    "                                        headers={\"Content-Type\": \"application/sdp\"}, timeout=15) as resp:\n",
    "                    if resp.status == 201:\n",
    "                        answer_sdp = await resp.text()\n",
    "                        await self.pc.setRemoteDescription(RTCSessionDescription(sdp=answer_sdp, type=\"answer\"))\n",
    "                    else:\n",
    "                        self.done.set()\n",
    "            await self.done.wait()\n",
    "        finally:\n",
    "            if 'display_task' in locals() and not display_task.done():\n",
    "                display_task.cancel()\n",
    "            if self.pc.connectionState != \"closed\":\n",
    "                await self.pc.close()\n",
    "\n",
    "async def main():\n",
    "    viewer = WebRTCStreamViewer(WHEP_URL, image_processor, depth_model, device, dtype)\n",
    "    await viewer.run()\n",
    "\n",
    "try:\n",
    "    await main()\n",
    "except (KeyboardInterrupt, asyncio.CancelledError):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb047af",
   "metadata": {},
   "source": [
    "More Optimized Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047fe4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch torchvision accelerate aiortc aiohttp Pillow --quiet\n",
    "\n",
    "import asyncio, cv2, numpy as np, aiohttp, logging, base64, torch, time\n",
    "from aiortc import RTCPeerConnection, RTCSessionDescription\n",
    "from aiortc.mediastreams import MediaStreamError\n",
    "from IPython.display import display, HTML\n",
    "from transformers import AutoImageProcessor, AutoModelForDepthEstimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbd7ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "WHEP_URL = \"https://(     your Url       )/video/whep\"\n",
    "DISPLAY_FPS = 30  \n",
    "\n",
    "logging.getLogger(\"libav\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"aiortc.codecs.h264\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "try:\n",
    "    checkpoint = \"LiheYoung/depth-anything-small-hf\"\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # Using float16 on CUDA.\n",
    "    dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
    "    print(f\"Loading model on device: '{device}' with dtype: {dtype}\")\n",
    "\n",
    "    image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "    depth_model = AutoModelForDepthEstimation.from_pretrained(\n",
    "        checkpoint,\n",
    "        torch_dtype=dtype,\n",
    "        # device_map=\"auto\" \n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    # if hasattr(torch, 'compile'):\n",
    "    #     print(\"Compiling model with torch.compile()... This may take a moment.\")\n",
    "    #     # This will JIT-compile the model on the first run for your specific hardware.\n",
    "    #     depth_model = torch.compile(depth_model, mode=\"max-autotune\")\n",
    "\n",
    "    print(\"Depth Pro model and processor loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load model: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "\n",
    "class WebRTCStreamViewer:\n",
    "    def __init__(self, whep_url, processor, model, device, dtype):\n",
    "        self.whep_url = whep_url\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.pc = RTCPeerConnection()\n",
    "        self.done = asyncio.Event()\n",
    "        self.lock = asyncio.Lock()\n",
    "        self.latest_frame = None\n",
    "        self.last_frame_timestamp = 0\n",
    "\n",
    "        @self.pc.on(\"track\")\n",
    "        async def on_track(track):\n",
    "            print(f\"Track {track.kind} received\")\n",
    "            if track.kind == \"video\":\n",
    "                asyncio.create_task(self._frame_receiver_task(track))\n",
    "\n",
    "        @self.pc.on(\"connectionstatechange\")\n",
    "        async def on_connectionstatechange():\n",
    "            print(f\"Connection state is {self.pc.connectionState}\")\n",
    "            if self.pc.connectionState in [\"failed\", \"closed\", \"disconnected\"]:\n",
    "                self.done.set()\n",
    "\n",
    "    async def _frame_receiver_task(self, track):\n",
    "        \"\"\"Receives frames and puts the latest one into a shared variable.\"\"\"\n",
    "        while not self.done.is_set():\n",
    "            try:\n",
    "                frame = await track.recv()\n",
    "                img = frame.to_ndarray(format=\"bgr24\")\n",
    "                async with self.lock:\n",
    "                    self.latest_frame = img\n",
    "                    self.last_frame_timestamp = frame.time\n",
    "            except MediaStreamError:\n",
    "                print(\"Stream ended\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                print(f\"Error in receiver task: {e}\")\n",
    "                return\n",
    "\n",
    "    async def _display_loop_task(self):\n",
    "        \"\"\"Processes and displays frames at a target FPS.\"\"\"\n",
    "        display_handle = display(HTML('<img>'), display_id=True)\n",
    "        target_delay = 1 / DISPLAY_FPS\n",
    "        last_processed_timestamp = 0\n",
    "\n",
    "        while not self.done.is_set():\n",
    "            start_time = time.perf_counter() \n",
    "\n",
    "            frame_to_process = None\n",
    "            current_frame_timestamp = 0\n",
    "\n",
    "            async with self.lock:\n",
    "                if self.latest_frame is not None and self.last_frame_timestamp > last_processed_timestamp:\n",
    "                    frame_to_process = self.latest_frame.copy()\n",
    "                    current_frame_timestamp = self.last_frame_timestamp\n",
    "                    last_processed_timestamp = current_frame_timestamp\n",
    "\n",
    "            if frame_to_process is not None:\n",
    "                original_h, original_w, _ = frame_to_process.shape\n",
    "\n",
    "                rgb_frame = cv2.cvtColor(frame_to_process, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                inputs = self.processor(images=rgb_frame, return_tensors=\"pt\")\n",
    "                pixel_values = inputs.pixel_values.to(self.device, self.dtype)\n",
    "\n",
    "                with torch.no_grad():       # with torch.inference_mode():    \n",
    "                    outputs = self.model(pixel_values)\n",
    "                    predicted_depth = outputs.predicted_depth\n",
    "\n",
    "                    #'bilinear' for faster interpolation.\n",
    "                    prediction = torch.nn.functional.interpolate(\n",
    "                        predicted_depth.unsqueeze(1),\n",
    "                        size=(original_h, original_w),\n",
    "                        mode=\"bilinear\", #  \"bicubic\"\n",
    "                        align_corners=False,\n",
    "                    )\n",
    "\n",
    "                    \n",
    "                    p_min = torch.min(prediction)\n",
    "                    p_max = torch.max(prediction)\n",
    "                    if p_max > p_min:\n",
    "                        normalized_prediction = (prediction - p_min) / (p_max - p_min)\n",
    "                    else:\n",
    "                        normalized_prediction = torch.zeros_like(prediction)\n",
    "                    \n",
    "                    \n",
    "                    output_normalized = (normalized_prediction.squeeze() * 255.0).cpu().to(torch.uint8).numpy()\n",
    "\n",
    "                depth_colormap = cv2.applyColorMap(output_normalized, cv2.COLORMAP_JET)\n",
    "\n",
    "                \n",
    "                combined_frame = np.hstack((frame_to_process, depth_colormap))\n",
    "\n",
    "                _, buffer = cv2.imencode('.jpg', combined_frame, [int(cv2.IMWRITE_JPEG_QUALITY), 80])\n",
    "                b64_str = base64.b64encode(buffer).decode('utf-8')\n",
    "                data_url = f\"data:image/jpeg;base64,{b64_str}\"\n",
    "                \n",
    "                \n",
    "                display_handle.update(HTML(f'<img src=\"{data_url}\" style=\"width: 80%;\" />'))\n",
    "\n",
    "            # Adaptive sleep for stable FPS\n",
    "            # change\n",
    "            processing_time = time.perf_counter() - start_time\n",
    "            sleep_duration = max(0, target_delay - processing_time)\n",
    "            await asyncio.sleep(sleep_duration)\n",
    "\n",
    "\n",
    "    async def run(self):\n",
    "        \"\"\"Main method to start the WebRTC connection and processing loops.\"\"\"\n",
    "        self.pc.addTransceiver(\"video\", direction=\"recvonly\")\n",
    "        offer = await self.pc.createOffer()\n",
    "        await self.pc.setLocalDescription(offer)\n",
    "\n",
    "        display_task = None\n",
    "        try:\n",
    "            display_task = asyncio.create_task(self._display_loop_task())\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                print(\"Attempting to connect to WHEP endpoint...\")\n",
    "                async with session.post(self.whep_url, data=self.pc.localDescription.sdp,\n",
    "                                        headers={\"Content-Type\": \"application/sdp\"}, timeout=15) as resp:\n",
    "                    if resp.status == 201:\n",
    "                        print(\"WHEP connection successful. Receiving answer...\")\n",
    "                        answer_sdp = await resp.text()\n",
    "                        await self.pc.setRemoteDescription(RTCSessionDescription(sdp=answer_sdp, type=\"answer\"))\n",
    "                    else:\n",
    "                        print(f\"WHEP connection failed with status {resp.status}: {await resp.text()}\")\n",
    "                        self.done.set()\n",
    "            \n",
    "            await self.done.wait()\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during run: {e}\")\n",
    "        finally:\n",
    "            print(\"Cleaning up...\")\n",
    "            if display_task and not display_task.done():\n",
    "                display_task.cancel()\n",
    "            if self.pc.connectionState != \"closed\":\n",
    "                await self.pc.close()\n",
    "            print(\"Cleanup complete.\")\n",
    "\n",
    "async def main():\n",
    "    viewer = WebRTCStreamViewer(WHEP_URL, image_processor, depth_model, device, dtype)\n",
    "    await viewer.run()\n",
    "\n",
    "\n",
    "try:\n",
    "    await main()\n",
    "except (KeyboardInterrupt, asyncio.CancelledError):\n",
    "    print(\"Stream stopped by user.\")\n",
    "finally:\n",
    "    print(\"Main task finished.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
