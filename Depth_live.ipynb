{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca49b26a",
   "metadata": {},
   "source": [
    "Trying out different models and their performance for the live depth mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b816a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch torchvision accelerate aiortc aiohttp Pillow --quiet\n",
    "\n",
    "\n",
    "import asyncio, cv2, numpy as np, aiohttp, logging, base64, torch\n",
    "from aiortc import RTCPeerConnection, RTCSessionDescription\n",
    "from aiortc.mediastreams import MediaStreamError\n",
    "from IPython.display import display, HTML\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n",
    "# from transformers import GLPNImageProcessor, GLPNForDepthEstimation      # For glpn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89194b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "WHEP_URL = \"https://(          Your Url          )/whep\"\n",
    "DISPLAY_FPS =  30   #  15\n",
    "\n",
    "logging.getLogger(\"libav\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"aiortc.codecs.h264\").setLevel(logging.ERROR)\n",
    "\n",
    "try:\n",
    "    checkpoint = \"LiheYoung/depth-anything-small-hf\"       \n",
    "    # \"vinvino02/glpn-kitti\"   \"vinvino02/glpn-nyu\"    \"apple/DepthPro-hf\"   \"LiheYoung/depth-anything-large-hf\"\n",
    "    # \"LiheYoung/depth-anything-small-hf\"    \"Intel/dpt-large\"\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
    "    print(f\"Loading model on device: '{device}' with dtype: {dtype}\")\n",
    "\n",
    "    image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "    depth_model = AutoModelForDepthEstimation.from_pretrained(\n",
    "        checkpoint,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    print(\"Model and processor loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63839581",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebRTCStreamViewer:\n",
    "    def __init__(self, whep_url, processor, model, device, dtype):\n",
    "        self.whep_url = whep_url\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.pc = RTCPeerConnection()\n",
    "        self.done = asyncio.Event()\n",
    "        self.lock = asyncio.Lock()\n",
    "        self.latest_frame = None\n",
    "\n",
    "        @self.pc.on(\"track\")\n",
    "        async def on_track(track):\n",
    "            if track.kind == \"video\":\n",
    "                asyncio.create_task(self._frame_receiver_task(track))\n",
    "\n",
    "        @self.pc.on(\"connectionstatechange\")\n",
    "        async def on_connectionstatechange():\n",
    "            if self.pc.connectionState in [\"failed\", \"closed\", \"disconnected\"]:\n",
    "                self.done.set()\n",
    "\n",
    "    async def _frame_receiver_task(self, track):\n",
    "        while not self.done.is_set():\n",
    "            try:\n",
    "                frame = await track.recv()\n",
    "                img = frame.to_ndarray(format=\"bgr24\")\n",
    "                async with self.lock:\n",
    "                    self.latest_frame = img\n",
    "            except MediaStreamError:\n",
    "                return\n",
    "\n",
    "    async def _display_loop_task(self):\n",
    "        display_handle = display(HTML('<img>'), display_id=True)\n",
    "\n",
    "        while not self.done.is_set():\n",
    "            frame_to_display = None\n",
    "            async with self.lock:\n",
    "                if self.latest_frame is not None:\n",
    "                    frame_to_display = self.latest_frame.copy()\n",
    "\n",
    "            if frame_to_display is not None:\n",
    "                original_h, original_w, _ = frame_to_display.shape\n",
    "                \n",
    "                rgb_image = Image.fromarray(cv2.cvtColor(frame_to_display, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "                inputs = self.processor(images=rgb_image, return_tensors=\"pt\")\n",
    "                pixel_values = inputs.pixel_values.to(self.device, dtype=self.dtype)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(pixel_values)\n",
    "                    predicted_depth = outputs.predicted_depth\n",
    "\n",
    "                prediction = torch.nn.functional.interpolate(\n",
    "                    predicted_depth.unsqueeze(1),\n",
    "                    size=(original_h, original_w),\n",
    "                    mode=\"bicubic\",\n",
    "                    align_corners=False,\n",
    "                ).squeeze()\n",
    "\n",
    "                output = prediction.cpu().numpy()\n",
    "                formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n",
    "                depth_colormap = cv2.applyColorMap(formatted, cv2.COLORMAP_J)\n",
    "\n",
    "                combined_frame = np.hstack((frame_to_display, depth_colormap))\n",
    "\n",
    "                _, buffer = cv2.imencode('.jpg', combined_frame)\n",
    "                b64_str = base64.b64encode(buffer).decode('utf-8')\n",
    "                data_url = f\"data:image/jpeg;base64,{b64_str}\"\n",
    "                display_handle.update(HTML(f'<img src=\"{data_url}\" style=\"width: 80%;\" />'))\n",
    "\n",
    "            await asyncio.sleep(1 / DISPLAY_FPS)\n",
    "\n",
    "    async def run(self):\n",
    "        self.pc.addTransceiver(\"video\", direction=\"recvonly\")\n",
    "        offer = await self.pc.createOffer()\n",
    "        await self.pc.setLocalDescription(offer)\n",
    "\n",
    "        try:\n",
    "            display_task = asyncio.create_task(self._display_loop_task())\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                async with session.post(self.whep_url, data=self.pc.localDescription.sdp,\n",
    "                                        headers={\"Content-Type\": \"application/sdp\"}, timeout=15) as resp:\n",
    "                    if resp.status == 201:\n",
    "                        answer_sdp = await resp.text()\n",
    "                        await self.pc.setRemoteDescription(RTCSessionDescription(sdp=answer_sdp, type=\"answer\"))\n",
    "                    else:\n",
    "                        self.done.set()\n",
    "            await self.done.wait()\n",
    "        finally:\n",
    "            if 'display_task' in locals() and not display_task.done():\n",
    "                display_task.cancel()\n",
    "            if self.pc.connectionState != \"closed\":\n",
    "                await self.pc.close()\n",
    "\n",
    "async def main():\n",
    "    viewer = WebRTCStreamViewer(WHEP_URL, image_processor, depth_model, device, dtype)\n",
    "    await viewer.run()\n",
    "\n",
    "try:\n",
    "    await main()\n",
    "except (KeyboardInterrupt, asyncio.CancelledError):\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
